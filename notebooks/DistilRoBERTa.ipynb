{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import comet_ml\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, confusion_matrix, classification_report,\n",
    "    balanced_accuracy_score, accuracy_score,\n",
    ")\n",
    "from transformers.integrations import CometCallback\n",
    "\n",
    "sys.path.append('..')\n",
    "from lib.utils import set_seed, get_device\n",
    "from lib.utils.constants import RANDOM_SEED\n",
    "from lib.data.loading import pd_read_jsonl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# comet_api_key = os.getenv(\"COMET_API_KEY\")\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train = pd_read_jsonl_file('../data/en_train.jsonl')\n",
    "df_en_test = pd_read_jsonl_file('../data/en_dev.jsonl')\n",
    "df_en_test_no_label = pd_read_jsonl_file('../data/en_devtest_text_id_only.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_en_train, df_en_dev = train_test_split(df_en_train, test_size=0.3, random_state=RANDOM_SEED, stratify=df_en_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 427536 entries, 386827 to 73522\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   id          427536 non-null  object\n",
      " 1   source      427536 non-null  object\n",
      " 2   sub_source  427536 non-null  object\n",
      " 3   lang        427536 non-null  object\n",
      " 4   model       427536 non-null  object\n",
      " 5   label       427536 non-null  int64 \n",
      " 6   text        427536 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 26.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_en_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sub_source</th>\n",
       "      <th>lang</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386827</th>\n",
       "      <td>c923eeb6-485f-426a-8d92-e097e7d314ff</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin Islander here.\\n\\nAs the wiki entry not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447629</th>\n",
       "      <td>f963f997-4bba-4ff7-9089-3eeb23b69f00</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>outfox</td>\n",
       "      <td>en</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>1</td>\n",
       "      <td>No I think that the Electoral College does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527410</th>\n",
       "      <td>035cdd43-a558-4071-b561-09ab5a046654</td>\n",
       "      <td>mage</td>\n",
       "      <td>yelp</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>Based on Yelp reviews, I thought I had found m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19183</th>\n",
       "      <td>18571e52-70d5-4fe8-8059-b388a48657c6</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to what you may have seen in western ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551034</th>\n",
       "      <td>4b6a8cb8-1d78-42a3-a5a3-ab863a7fb844</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>That's a bad question. It's like asking \"how b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id source sub_source lang  \\\n",
       "386827  c923eeb6-485f-426a-8d92-e097e7d314ff   m4gt     reddit   en   \n",
       "447629  f963f997-4bba-4ff7-9089-3eeb23b69f00   m4gt     outfox   en   \n",
       "527410  035cdd43-a558-4071-b561-09ab5a046654   mage       yelp   en   \n",
       "19183   18571e52-70d5-4fe8-8059-b388a48657c6   m4gt     reddit   en   \n",
       "551034  4b6a8cb8-1d78-42a3-a5a3-ab863a7fb844   m4gt     reddit   en   \n",
       "\n",
       "                model  label  \\\n",
       "386827          human      0   \n",
       "447629      llama3-8b      1   \n",
       "527410          human      0   \n",
       "19183   gpt-3.5-turbo      1   \n",
       "551034          human      0   \n",
       "\n",
       "                                                     text  \n",
       "386827  Virgin Islander here.\\n\\nAs the wiki entry not...  \n",
       "447629  No I think that the Electoral College does not...  \n",
       "527410  Based on Yelp reviews, I thought I had found m...  \n",
       "19183   Contrary to what you may have seen in western ...  \n",
       "551034  That's a bad question. It's like asking \"how b...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert/distilroberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406309fc34144f85be87bcea1e993984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e0508efa264cba8fa0868cbbd53b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ca85c309354f80998490b493b0d364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd0c9cdb69c435184e8bc6091250626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea4db9c34049a4aa0fd1c0925ca2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1457 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of tokens in each sentence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      4\u001b[0m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of tokens in each sentence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      4\u001b[0m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2538\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2626\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2645\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2646\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2647\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2648\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2649\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2650\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2651\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2652\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2653\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2654\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2655\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2656\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2657\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2658\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2659\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2660\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2661\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2662\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2663\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2709\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2710\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2714\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2715\u001b[0m )\n\u001b[0;32m-> 2717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2718\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2719\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2720\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2721\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2722\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2723\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2724\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2725\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2726\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2727\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2728\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2729\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2730\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2731\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2732\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2733\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2734\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2735\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2736\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:275\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_encode_plus(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    480\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    498\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    499\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 500\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    501\u001b[0m         batched_input,\n\u001b[1;32m    502\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    503\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    504\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    505\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    506\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    507\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    508\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    509\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    510\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    511\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    512\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    513\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    514\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    515\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    516\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:265\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m )\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    421\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    422\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    426\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    429\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    430\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    431\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    440\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    442\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    452\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Count the number of tokens in each sentence\n",
    "df_en_train['num_tokens'] = df_en_train['text'].apply(lambda x: len(tokenizer(x)['input_ids']))\n",
    "df_en_dev['num_tokens'] = df_en_dev['text'].apply(lambda x: len(tokenizer(x)['input_ids']))\n",
    "df_en_test['num_tokens'] = df_en_test['text'].apply(lambda x: len(tokenizer(x)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_dev['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_test['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot over the number of tokens grouped by dataset and label\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(25, 10))\n",
    "\n",
    "sns.histplot(df_en_train, x='num_tokens', hue='label', kde=True, ax=ax[0][0])\n",
    "sns.histplot(df_en_dev, x='num_tokens', hue='label', kde=True, ax=ax[0][1])\n",
    "sns.histplot(df_en_test, x='num_tokens', kde=True, ax=ax[0][2])\n",
    "\n",
    "sns.histplot(df_en_train, x='num_tokens', hue='label', kde=True, ax=ax[1][0])\n",
    "sns.histplot(df_en_dev, x='num_tokens', hue='label', kde=True, ax=ax[1][1])\n",
    "sns.histplot(df_en_test, x='num_tokens', kde=True, ax=ax[1][2])\n",
    "\n",
    "ax[0][0].set_title('Train')\n",
    "ax[0][0].set_xlim(left=-50)\n",
    "ax[0][1].set_title('Dev')\n",
    "ax[0][1].set_xlim(left=-50)\n",
    "ax[0][2].set_title('Test')\n",
    "ax[0][2].set_xlim(left=-50)\n",
    "\n",
    "ax[1][0].set_title('Train')\n",
    "ax[1][0].set_xlim(left=-10, right=2500)\n",
    "ax[1][1].set_title('Dev')\n",
    "ax[1][1].set_xlim(left=-10, right=2500)\n",
    "ax[1][2].set_title('Test')\n",
    "ax[1][2].set_xlim(left=-10, right=2500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert/distilroberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.roberta.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 7680002 (9.35%)\n"
     ]
    }
   ],
   "source": [
    "def get_num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "trainable_params = get_num_parameters(model)\n",
    "print(f'Trainable parameters: {trainable_params} ({trainable_params / model.num_parameters():.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Measure performance before training/fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8180/8180 [08:46<00:00, 15.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.38856221156147674,\n",
       " 'accuracy': 0.6211347886215512,\n",
       " 'balanced_accuracy': 0.498585638605751,\n",
       " 'confusion_matrix': array([[   575,  97753],\n",
       "        [  1418, 162012]]),\n",
       " 'classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.29      0.01      0.01     98328\\n           1       0.62      0.99      0.77    163430\\n\\n    accuracy                           0.62    261758\\n   macro avg       0.46      0.50      0.39    261758\\nweighted avg       0.50      0.62      0.48    261758\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_performance_metrics(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cr\n",
    "    }\n",
    "\n",
    "\n",
    "def measure_initial_performance(model, tokenizer, df_test, device, max_length=512, batch_size=32):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_samples = df_test.text.tolist()\n",
    "\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, len(test_samples), batch_size)):\n",
    "        batch = test_samples[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            outputs.append(output.logits)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    performance = get_performance_metrics(df_test.label, predictions)\n",
    "    return performance\n",
    "\n",
    "\n",
    "initial_performance = measure_initial_performance(model, tokenizer, df_en_dev, DEVICE)\n",
    "initial_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_samples(model, tokenizer, df, device, max_length=512):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_samples = df.text.tolist()[:5]\n",
    "    inputs = tokenizer(\n",
    "        test_samples,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0314,  0.0181],\n",
      "        [-0.0319,  0.0190],\n",
      "        [-0.0338,  0.0227],\n",
      "        [-0.0318,  0.0197],\n",
      "        [-0.0323,  0.0195]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_samples(model, tokenizer, df_en_train, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6252, 0.3748])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (\n",
    "    1 / df_en_train.label.value_counts(normalize=True).sort_index()\n",
    ").tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_en_train)\n",
    "ds_dev = Dataset.from_pandas(df_en_dev)\n",
    "ds_test = Dataset.from_pandas(df_en_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "    num_rows: 427536\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_shuffled = ds_train.shuffle(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': ds_train_shuffled,\n",
    "    'dev': ds_dev,\n",
    "    'test': ds_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "        num_rows: 427536\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "        num_rows: 183231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2befe825ae43239f95d5add835815c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/427536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31815fd2a58145e297758610048f0052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/183231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf540451324890996c740aaa4a9e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/261758 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "# col_to_delete = ['source', 'sub_source', 'lang', 'model']\n",
    "\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    return tokenizer(ds['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_dataset, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 427536\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 183231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'macro_f1': f1_score(predictions, labels, average='macro'),\n",
    "        'accuracy': accuracy_score(predictions,labels),\n",
    "        'balanced_accuracy': balanced_accuracy_score(predictions, labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class WeightedCrossEntropyTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(\n",
    "                class_weights, dtype=torch.float32\n",
    "        ).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "# from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "# experiment = Experiment(\n",
    "#     api_key=comet_api_key,\n",
    "#     project_name=\"mgt-detection-coling-2025\",\n",
    "#     workspace=\"unibuc-phd\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"seed\": RANDOM_SEED,\n",
    "}\n",
    "\n",
    "# experiment.log_parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results/subtask_a/002_distilroberta-base',\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=params[\"num_train_epochs\"],\n",
    "    weight_decay=params[\"weight_decay\"],\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    # report_to=\"comet_ml\",\n",
    "    seed=params[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14841/3314886341.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedCrossEntropyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['dev'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    # callbacks=[CometCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26722' max='26722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26722/26722 58:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.369481</td>\n",
       "      <td>0.900158</td>\n",
       "      <td>0.910283</td>\n",
       "      <td>0.925452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.306249</td>\n",
       "      <td>0.920375</td>\n",
       "      <td>0.927692</td>\n",
       "      <td>0.938710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.0\n",
      "  total_flos               = 105462933GF\n",
      "  train_loss               =      0.1341\n",
      "  train_runtime            =  0:58:24.95\n",
      "  train_samples            =      427536\n",
      "  train_samples_per_second =     243.961\n",
      "  train_steps_per_second   =       7.624\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(ds_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(ds_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../results/subtask_a/002_distilroberta-base/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def make_predictions(model, tokenizer, df, device, file_path=None, max_len=MAX_LEN, batch_size=16):\n",
    "    model.eval()\n",
    "\n",
    "    has_labels = (\"label\" in df.columns)\n",
    "\n",
    "    all_ids = []\n",
    "    all_true = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(df), batch_size)):\n",
    "            ids = df[i:i + batch_size].id.tolist()\n",
    "            texts = df[i:i + batch_size].text.tolist()\n",
    "            labels = None\n",
    "            if has_labels:\n",
    "                labels = df[i:i + batch_size].label.tolist()\n",
    "\n",
    "            inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            all_ids.extend(ids)\n",
    "            if has_labels:\n",
    "                all_true.extend(labels)\n",
    "            all_predictions.extend(preds)\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"label\": all_predictions,\n",
    "    })\n",
    "\n",
    "    if has_labels:\n",
    "        df_predictions[\"true\"] = all_true\n",
    "\n",
    "    if file_path is not None:\n",
    "        df_predictions.to_json(\n",
    "            file_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 11026/16360 [09:13<04:16, 20.79it/s]"
     ]
    }
   ],
   "source": [
    "df_test_predictions = make_predictions(\n",
    "    trainer.model,\n",
    "    tokenizer,\n",
    "    df_en_test,\n",
    "    DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.625189\n",
       "0    0.374811\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.677805\n",
       "0    0.322195\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predictions.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9204445311773776\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for test dataset\n",
    "print(\n",
    "    f1_score(df_test_predictions.true, df_test_predictions.label, average='macro')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2035/2035 [01:47<00:00, 18.86it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test_submission_predictions = make_predictions(\n",
    "    trainer.model,\n",
    "    tokenizer,\n",
    "    df_en_test_no_label,\n",
    "    DEVICE,\n",
    "    \"../results/subtask_a/002_distilroberta-base/subtask_a.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.774795\n",
       "0    0.225205\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_submission_predictions.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00f4d4e9c768461aaf3eb69fc4f355ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1bb23128fd0d46f7bcd2caf6701f54cd",
       "style": "IPY_MODEL_f4062d36264047de804590f74a5e1b8b",
       "value": "Map: 100%"
      }
     },
     "04facc1fbddc40489bc1f55922d76d54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7f5e3f06d4cc455ea88605a514a969e7",
       "style": "IPY_MODEL_30bf8a87a067417f9c0535912283f8d6",
       "value": "config.json: 100%"
      }
     },
     "06e698a520da4a638fc463fa0471abcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "078be928f9824d19827baf252300baaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0abf540451324890996c740aaa4a9e93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_00f4d4e9c768461aaf3eb69fc4f355ce",
        "IPY_MODEL_ca828c798a7441da8c1c9cc6e68e6f14",
        "IPY_MODEL_7d65445a99034dce9f5afae88bf6a575"
       ],
       "layout": "IPY_MODEL_a162888bda6e449b9827b98b324fff24"
      }
     },
     "1009087a1c5a4cd58c681a635a303b00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3905d834588c47ca935580f85a0e0955",
       "style": "IPY_MODEL_63fb20bc72284774a62477d42cf52149",
       "value": " 261758/261758 [00:59&lt;00:00, 4461.17 examples/s]"
      }
     },
     "12a33062e62542989ecec46568e2dd9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2337b73307704d0ea9048ca2d4b494c8",
       "max": 1355863,
       "style": "IPY_MODEL_cd978a9a37424428b87121ddd8e00b4d",
       "value": 1355863
      }
     },
     "135eac23d49449469fdc772e12b118d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1785649cb01141e3bd42b710347bfeb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_db0a63ac61494f77986f448d4be16a89",
       "style": "IPY_MODEL_27131055865341728f80863c0f3dc0fe",
       "value": " 1.36M/1.36M [00:00&lt;00:00, 2.13MB/s]"
      }
     },
     "1bb23128fd0d46f7bcd2caf6701f54cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1beb52f18cef4e3a9302cb361a25827e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f232458cd1b4489a76172b91061507a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1fd0c9cdb69c435184e8bc6091250626": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_66d24f3ee91b490e8759718b14352d26",
        "IPY_MODEL_bdb66aeb7cf54166999b8cdb73ffdc8e",
        "IPY_MODEL_9cc9a74ea820472f809df8f975935067"
       ],
       "layout": "IPY_MODEL_9ebf891b907f4c24a1427c1dbd10edd0"
      }
     },
     "22dc590e976741febfbd4a0bebd08552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b18063c17fb74e3b8a6ad218180bb735",
        "IPY_MODEL_cbfe31b0672e4ea0808b941c03d72c9f",
        "IPY_MODEL_6b76a598a8bf4ecba3c78a3d0e27037c"
       ],
       "layout": "IPY_MODEL_42157220e84b4e43ba82b95166bc3086"
      }
     },
     "2331f0eefa7c4c75b1f2848739cb5d9a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2337b73307704d0ea9048ca2d4b494c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23adfbb236d5469fa72b55479de468d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e51d8e0c59fc4b8b8395357786bcdb25",
       "style": "IPY_MODEL_89f6aaa25ce94688b6d0040dbf111a06",
       "value": "Map: 100%"
      }
     },
     "26124d0feb564a709dfa860183fdd719": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a8a6bb3105d44482b534068a3f425279",
       "max": 331055963,
       "style": "IPY_MODEL_3d5ff106fae24db498bc19092f905f7f",
       "value": 331055963
      }
     },
     "27131055865341728f80863c0f3dc0fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2edfef4d2d4946c4adad0be6c457487c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ff7b0ecdce94fc18132f7c197cd188a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b3d2fffd93bd4796bd54621b96632094",
       "style": "IPY_MODEL_1f232458cd1b4489a76172b91061507a",
       "value": "Map: 100%"
      }
     },
     "301cd086af884ed0afc9ef3ae963aa05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ab98bab9f0aa40bcacbd34e28b1daacb",
       "style": "IPY_MODEL_f2cd4536f6404ae8b75bda4dbceec05a",
       "value": " 331M/331M [00:02&lt;00:00, 129MB/s]"
      }
     },
     "303a953a75ae43e3ab8b0d8041e6785b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e730cb2709b74f3085787150a4c52742",
       "style": "IPY_MODEL_7a00a4a6391748389c2019d6154a3b31",
       "value": "model.safetensors: 100%"
      }
     },
     "30bf8a87a067417f9c0535912283f8d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31815fd2a58145e297758610048f0052": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_23adfbb236d5469fa72b55479de468d5",
        "IPY_MODEL_63be81bb3732461292cc7c3adf96daa8",
        "IPY_MODEL_61b8c31fb9db49769234beefb7a23cf4"
       ],
       "layout": "IPY_MODEL_4392ba410d0842959df4b2bbc0b04724"
      }
     },
     "335d8a0391504646a016d00a50e914ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4dc4810887a94184873c8ff1acbc20e7",
       "max": 32557,
       "style": "IPY_MODEL_953dd9c0626e4748a63c885d869f25d0",
       "value": 32557
      }
     },
     "3905d834588c47ca935580f85a0e0955": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3b1aa08eff34418d92a130764f2bdd33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3d5ff106fae24db498bc19092f905f7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3f131e92a4fd4fe4a83cc3b8a524356e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_72b2e95fac7d4d75931e5bddb960b445",
       "max": 427536,
       "style": "IPY_MODEL_d8abd596d79b4715989ae5ca8afe61ef",
       "value": 427536
      }
     },
     "406309fc34144f85be87bcea1e993984": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e296a4fab51544b19837c38a1dab65c3",
        "IPY_MODEL_69f0497347e642269ed149660499e7a7",
        "IPY_MODEL_77fd2ea99eed416d836798bf7e5bd04a"
       ],
       "layout": "IPY_MODEL_c8e293928062434d83ea488d7a5949f2"
      }
     },
     "42157220e84b4e43ba82b95166bc3086": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42923a654b89497cb0e6bbeac0b3d1b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4392ba410d0842959df4b2bbc0b04724": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45553705f7ea49ed81d8f95594dbb1bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "468b246dbdd540f0a8584c844b5cc2cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "476bae227dbd4590aa0bd6390439d0fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a184b9a6ceb41eb984a7fde82071fc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_303a953a75ae43e3ab8b0d8041e6785b",
        "IPY_MODEL_26124d0feb564a709dfa860183fdd719",
        "IPY_MODEL_301cd086af884ed0afc9ef3ae963aa05"
       ],
       "layout": "IPY_MODEL_6873ab44d1e64050a4cefc1c79ae511f"
      }
     },
     "4c1971033cf44a6da458051c1730d441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4dc4810887a94184873c8ff1acbc20e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "56187d2e545a474087a396f0effd2641": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56f73c3ea4634f528959b3431512da10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "588fb3f87d6944c9bedd216065bec128": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5aa81bc8c97043ceb9c4ccf9aa609292": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "60471c0decaa48299810a26768d5a221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "61b8c31fb9db49769234beefb7a23cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a9b5a7c3ce254de5af4c846ba8f3b204",
       "style": "IPY_MODEL_af92c59cf13c4e92a8307427eacd8cd5",
       "value": " 183231/183231 [00:42&lt;00:00, 4436.99 examples/s]"
      }
     },
     "63be81bb3732461292cc7c3adf96daa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cf18d39b6cf0441191ac60d8492e1d85",
       "max": 183231,
       "style": "IPY_MODEL_7dd0d67f6536436fb7da3fa3acc180dd",
       "value": 183231
      }
     },
     "63fb20bc72284774a62477d42cf52149": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "66d24f3ee91b490e8759718b14352d26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e673c1fa455e4e64a89722d7a7e22039",
       "style": "IPY_MODEL_b12530b41410480593b4f2725263de30",
       "value": "merges.txt: 100%"
      }
     },
     "6873ab44d1e64050a4cefc1c79ae511f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68d9e91cd1004fc5aa2b7ea0ea810feb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "69f0497347e642269ed149660499e7a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_db25c0fb3b264d3dab22aa285d607279",
       "max": 25,
       "style": "IPY_MODEL_71c66ce1a7fe438abfb7effea3d87ea9",
       "value": 25
      }
     },
     "6b76a598a8bf4ecba3c78a3d0e27037c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e39e30aefafe4f50811f7ba5926ddb3b",
       "style": "IPY_MODEL_42923a654b89497cb0e6bbeac0b3d1b3",
       "value": " 610767/610767 [02:37&lt;00:00, 3885.10 examples/s]"
      }
     },
     "6e2befe825ae43239f95d5add835815c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_78a46e271f794678a404ce55acad088f",
        "IPY_MODEL_3f131e92a4fd4fe4a83cc3b8a524356e",
        "IPY_MODEL_fcb3bbc9be3349819dd4aac7eaa1f145"
       ],
       "layout": "IPY_MODEL_a45aeb66be1d4a3a8d6ff22292a3977f"
      }
     },
     "70e0508efa264cba8fa0868cbbd53b6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_04facc1fbddc40489bc1f55922d76d54",
        "IPY_MODEL_d2cabc26b027497692df6d7699e26914",
        "IPY_MODEL_adca228ceb4142e7b578bc6e78579068"
       ],
       "layout": "IPY_MODEL_89112f6b5986400e8ebcb66840e4ce80"
      }
     },
     "716fbbe17b644adb807d9bb7dc2de97f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "71c66ce1a7fe438abfb7effea3d87ea9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "72b2e95fac7d4d75931e5bddb960b445": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "73958c23f9cc46568e7d02e750afa389": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "74554611642d4be492445f1a6ce7b91a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74ca85c309354f80998490b493b0d364": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a4e2f4a66fe84e80a7c628b4948e5cc0",
        "IPY_MODEL_99fe73992a864dc580076dc51ba67d2c",
        "IPY_MODEL_b7d29fad914d4105a02e1dcb1a444ee8"
       ],
       "layout": "IPY_MODEL_fb320a6634c34fb1b570cfff57af9d3a"
      }
     },
     "7593710373954026adf089076faa8bf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "767520f1a0574310854f4a0d405f70a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "76a8180b91bc44a8a1f09fb91b18746b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2ff7b0ecdce94fc18132f7c197cd188a",
        "IPY_MODEL_335d8a0391504646a016d00a50e914ff",
        "IPY_MODEL_e1cf16b19a614953b4662ed622344b78"
       ],
       "layout": "IPY_MODEL_06e698a520da4a638fc463fa0471abcd"
      }
     },
     "76a9bf394f6a43038c73e4d8ef3c5ed1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "77fd2ea99eed416d836798bf7e5bd04a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e30ae229ed7d41a6b6504f8147ffeb04",
       "style": "IPY_MODEL_b2f45d6d06a2404d9ea04466c37e723e",
       "value": " 25.0/25.0 [00:00&lt;00:00, 860B/s]"
      }
     },
     "78a46e271f794678a404ce55acad088f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_135eac23d49449469fdc772e12b118d4",
       "style": "IPY_MODEL_74554611642d4be492445f1a6ce7b91a",
       "value": "Map: 100%"
      }
     },
     "7a00a4a6391748389c2019d6154a3b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7d65445a99034dce9f5afae88bf6a575": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9bd9e62bb9f647fb83bc20c2c674d4b7",
       "style": "IPY_MODEL_56187d2e545a474087a396f0effd2641",
       "value": " 261758/261758 [01:03&lt;00:00, 4256.01 examples/s]"
      }
     },
     "7dd0d67f6536436fb7da3fa3acc180dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7ddb3cd0f58647998b414e4424d59f48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7f5e3f06d4cc455ea88605a514a969e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "81ca028c452a4d6085338083731aadfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "82aad2e3658c48bdb65fdf85fa7cb185": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "89112f6b5986400e8ebcb66840e4ce80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89f6aaa25ce94688b6d0040dbf111a06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "953dd9c0626e4748a63c885d869f25d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "99fe73992a864dc580076dc51ba67d2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1beb52f18cef4e3a9302cb361a25827e",
       "max": 898823,
       "style": "IPY_MODEL_b7dd65c424c247b1bf78aa3b535afd24",
       "value": 898823
      }
     },
     "9bd9e62bb9f647fb83bc20c2c674d4b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9cc9a74ea820472f809df8f975935067": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7ddb3cd0f58647998b414e4424d59f48",
       "style": "IPY_MODEL_fb4784f819d3423b8a411c5121266705",
       "value": " 456k/456k [00:00&lt;00:00, 1.17MB/s]"
      }
     },
     "9ebf891b907f4c24a1427c1dbd10edd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a018e29e292840b8b21c5f6b0f314b2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a04345620b3e421ba33416f4e7935d84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a162888bda6e449b9827b98b324fff24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2f6d230886a4a3898fc507f2230b07b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c73cb357a60146d98c61f9b13db12146",
        "IPY_MODEL_ae6a0ada5fc041f09d789c744a665735",
        "IPY_MODEL_1009087a1c5a4cd58c681a635a303b00"
       ],
       "layout": "IPY_MODEL_a8d2c921b4d84853be441bdd974eda71"
      }
     },
     "a45aeb66be1d4a3a8d6ff22292a3977f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a4e2f4a66fe84e80a7c628b4948e5cc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_60471c0decaa48299810a26768d5a221",
       "style": "IPY_MODEL_81ca028c452a4d6085338083731aadfe",
       "value": "vocab.json: 100%"
      }
     },
     "a8a6bb3105d44482b534068a3f425279": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a8d2c921b4d84853be441bdd974eda71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a93f863b558b49c7826eb74ded163b49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a9b5a7c3ce254de5af4c846ba8f3b204": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ab98bab9f0aa40bcacbd34e28b1daacb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "acea70b8a3554207973549f014afe888": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "adca228ceb4142e7b578bc6e78579068": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_73958c23f9cc46568e7d02e750afa389",
       "style": "IPY_MODEL_45553705f7ea49ed81d8f95594dbb1bf",
       "value": " 480/480 [00:00&lt;00:00, 18.2kB/s]"
      }
     },
     "ae6a0ada5fc041f09d789c744a665735": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2331f0eefa7c4c75b1f2848739cb5d9a",
       "max": 261758,
       "style": "IPY_MODEL_a93f863b558b49c7826eb74ded163b49",
       "value": 261758
      }
     },
     "af92c59cf13c4e92a8307427eacd8cd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b12530b41410480593b4f2725263de30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b18063c17fb74e3b8a6ad218180bb735": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a04345620b3e421ba33416f4e7935d84",
       "style": "IPY_MODEL_82aad2e3658c48bdb65fdf85fa7cb185",
       "value": "Map: 100%"
      }
     },
     "b2f45d6d06a2404d9ea04466c37e723e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b3d2fffd93bd4796bd54621b96632094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5ea4db9c34049a4aa0fd1c0925ca2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f3a70f5f36034d0292b940528994b520",
        "IPY_MODEL_12a33062e62542989ecec46568e2dd9a",
        "IPY_MODEL_1785649cb01141e3bd42b710347bfeb1"
       ],
       "layout": "IPY_MODEL_2edfef4d2d4946c4adad0be6c457487c"
      }
     },
     "b69ebf5d4bc5400b9665d989c2b9dd3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b7d29fad914d4105a02e1dcb1a444ee8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b69ebf5d4bc5400b9665d989c2b9dd3f",
       "style": "IPY_MODEL_dd6b752b2b7544979bbd915721f672d2",
       "value": " 899k/899k [00:00&lt;00:00, 1.73MB/s]"
      }
     },
     "b7dd65c424c247b1bf78aa3b535afd24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bdb66aeb7cf54166999b8cdb73ffdc8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5aa81bc8c97043ceb9c4ccf9aa609292",
       "max": 456318,
       "style": "IPY_MODEL_468b246dbdd540f0a8584c844b5cc2cf",
       "value": 456318
      }
     },
     "c73cb357a60146d98c61f9b13db12146": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fbdacac04b704874ae22e951f11c2573",
       "style": "IPY_MODEL_76a9bf394f6a43038c73e4d8ef3c5ed1",
       "value": "Map: 100%"
      }
     },
     "c8e293928062434d83ea488d7a5949f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca828c798a7441da8c1c9cc6e68e6f14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_588fb3f87d6944c9bedd216065bec128",
       "max": 261758,
       "style": "IPY_MODEL_767520f1a0574310854f4a0d405f70a5",
       "value": 261758
      }
     },
     "cbfe31b0672e4ea0808b941c03d72c9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_56f73c3ea4634f528959b3431512da10",
       "max": 610767,
       "style": "IPY_MODEL_a018e29e292840b8b21c5f6b0f314b2a",
       "value": 610767
      }
     },
     "cd978a9a37424428b87121ddd8e00b4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cf18d39b6cf0441191ac60d8492e1d85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d2cabc26b027497692df6d7699e26914": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_078be928f9824d19827baf252300baaa",
       "max": 480,
       "style": "IPY_MODEL_68d9e91cd1004fc5aa2b7ea0ea810feb",
       "value": 480
      }
     },
     "d8abd596d79b4715989ae5ca8afe61ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "db0a63ac61494f77986f448d4be16a89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db25c0fb3b264d3dab22aa285d607279": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db5abe3b80cd4b5d870d9c6d7ef6014e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd6b752b2b7544979bbd915721f672d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e1cf16b19a614953b4662ed622344b78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_db5abe3b80cd4b5d870d9c6d7ef6014e",
       "style": "IPY_MODEL_716fbbe17b644adb807d9bb7dc2de97f",
       "value": " 32557/32557 [00:10&lt;00:00, 3036.79 examples/s]"
      }
     },
     "e296a4fab51544b19837c38a1dab65c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_acea70b8a3554207973549f014afe888",
       "style": "IPY_MODEL_476bae227dbd4590aa0bd6390439d0fc",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "e30ae229ed7d41a6b6504f8147ffeb04": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e39e30aefafe4f50811f7ba5926ddb3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e51d8e0c59fc4b8b8395357786bcdb25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e673c1fa455e4e64a89722d7a7e22039": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e730cb2709b74f3085787150a4c52742": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f2cd4536f6404ae8b75bda4dbceec05a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f3a70f5f36034d0292b940528994b520": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7593710373954026adf089076faa8bf5",
       "style": "IPY_MODEL_f95e2614899c42dcb202ea7c5f483a41",
       "value": "tokenizer.json: 100%"
      }
     },
     "f4062d36264047de804590f74a5e1b8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f95e2614899c42dcb202ea7c5f483a41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fb320a6634c34fb1b570cfff57af9d3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb4784f819d3423b8a411c5121266705": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbdacac04b704874ae22e951f11c2573": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fcb3bbc9be3349819dd4aac7eaa1f145": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4c1971033cf44a6da458051c1730d441",
       "style": "IPY_MODEL_3b1aa08eff34418d92a130764f2bdd33",
       "value": " 427536/427536 [02:00&lt;00:00, 3679.13 examples/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
