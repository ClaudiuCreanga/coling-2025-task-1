{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import comet_ml\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, confusion_matrix, classification_report,\n",
    "    balanced_accuracy_score, accuracy_score,\n",
    ")\n",
    "from transformers.integrations import CometCallback\n",
    "\n",
    "sys.path.append('..')\n",
    "from lib.utils import set_seed, get_device\n",
    "from lib.utils.constants import RANDOM_SEED\n",
    "from lib.data.loading import pd_read_jsonl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# comet_api_key = os.getenv(\"COMET_API_KEY\")\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train = pd_read_jsonl_file('../data/en_train.jsonl')\n",
    "df_en_test = pd_read_jsonl_file('../data/en_dev.jsonl')\n",
    "df_en_test_no_label = pd_read_jsonl_file('../data/en_devtest_text_id_only.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_en_train, df_en_dev = train_test_split(df_en_train, test_size=0.3, random_state=RANDOM_SEED, stratify=df_en_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 427536 entries, 386827 to 73522\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   id          427536 non-null  object\n",
      " 1   source      427536 non-null  object\n",
      " 2   sub_source  427536 non-null  object\n",
      " 3   lang        427536 non-null  object\n",
      " 4   model       427536 non-null  object\n",
      " 5   label       427536 non-null  int64 \n",
      " 6   text        427536 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 26.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_en_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sub_source</th>\n",
       "      <th>lang</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386827</th>\n",
       "      <td>c923eeb6-485f-426a-8d92-e097e7d314ff</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin Islander here.\\n\\nAs the wiki entry not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447629</th>\n",
       "      <td>f963f997-4bba-4ff7-9089-3eeb23b69f00</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>outfox</td>\n",
       "      <td>en</td>\n",
       "      <td>llama3-8b</td>\n",
       "      <td>1</td>\n",
       "      <td>No I think that the Electoral College does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527410</th>\n",
       "      <td>035cdd43-a558-4071-b561-09ab5a046654</td>\n",
       "      <td>mage</td>\n",
       "      <td>yelp</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>Based on Yelp reviews, I thought I had found m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19183</th>\n",
       "      <td>18571e52-70d5-4fe8-8059-b388a48657c6</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to what you may have seen in western ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551034</th>\n",
       "      <td>4b6a8cb8-1d78-42a3-a5a3-ab863a7fb844</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>reddit</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>That's a bad question. It's like asking \"how b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id source sub_source lang  \\\n",
       "386827  c923eeb6-485f-426a-8d92-e097e7d314ff   m4gt     reddit   en   \n",
       "447629  f963f997-4bba-4ff7-9089-3eeb23b69f00   m4gt     outfox   en   \n",
       "527410  035cdd43-a558-4071-b561-09ab5a046654   mage       yelp   en   \n",
       "19183   18571e52-70d5-4fe8-8059-b388a48657c6   m4gt     reddit   en   \n",
       "551034  4b6a8cb8-1d78-42a3-a5a3-ab863a7fb844   m4gt     reddit   en   \n",
       "\n",
       "                model  label  \\\n",
       "386827          human      0   \n",
       "447629      llama3-8b      1   \n",
       "527410          human      0   \n",
       "19183   gpt-3.5-turbo      1   \n",
       "551034          human      0   \n",
       "\n",
       "                                                     text  \n",
       "386827  Virgin Islander here.\\n\\nAs the wiki entry not...  \n",
       "447629  No I think that the Electoral College does not...  \n",
       "527410  Based on Yelp reviews, I thought I had found m...  \n",
       "19183   Contrary to what you may have seen in western ...  \n",
       "551034  That's a bad question. It's like asking \"how b...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"allenai/longformer-base-4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406309fc34144f85be87bcea1e993984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e0508efa264cba8fa0868cbbd53b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ca85c309354f80998490b493b0d364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd0c9cdb69c435184e8bc6091250626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea4db9c34049a4aa0fd1c0925ca2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1457 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of tokens in each sentence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      4\u001b[0m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of tokens in each sentence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      4\u001b[0m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_en_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(tokenizer(x)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2538\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2626\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2645\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2646\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2647\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2648\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2649\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2650\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2651\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2652\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2653\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2654\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2655\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2656\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2657\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2658\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2659\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2660\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2661\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2662\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2663\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2709\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2710\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2714\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2715\u001b[0m )\n\u001b[0;32m-> 2717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2718\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2719\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2720\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2721\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2722\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2723\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2724\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2725\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2726\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2727\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2728\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2729\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2730\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2731\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2732\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2733\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2734\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2735\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2736\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:275\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_encode_plus(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    480\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    498\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    499\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 500\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    501\u001b[0m         batched_input,\n\u001b[1;32m    502\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    503\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    504\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    505\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    506\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    507\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    508\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    509\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    510\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    511\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    512\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    513\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    514\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    515\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    516\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:265\u001b[0m, in \u001b[0;36mRobertaTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m )\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    421\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    422\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    426\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    429\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    430\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    431\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    440\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    442\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    452\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Count the number of tokens in each sentence\n",
    "df_en_train['num_tokens'] = df_en_train['text'].apply(lambda x: len(tokenizer(x)['input_ids']))\n",
    "df_en_dev['num_tokens'] = df_en_dev['text'].apply(lambda x: len(tokenizer(x)['input_ids']))\n",
    "df_en_test['num_tokens'] = df_en_test['text'].apply(lambda x: len(tokenizer(x)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_dev['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_test['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot over the number of tokens grouped by dataset and label\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(25, 10))\n",
    "\n",
    "sns.histplot(df_en_train, x='num_tokens', hue='label', kde=True, ax=ax[0][0])\n",
    "sns.histplot(df_en_dev, x='num_tokens', hue='label', kde=True, ax=ax[0][1])\n",
    "sns.histplot(df_en_test, x='num_tokens', kde=True, ax=ax[0][2])\n",
    "\n",
    "sns.histplot(df_en_train, x='num_tokens', hue='label', kde=True, ax=ax[1][0])\n",
    "sns.histplot(df_en_dev, x='num_tokens', hue='label', kde=True, ax=ax[1][1])\n",
    "sns.histplot(df_en_test, x='num_tokens', kde=True, ax=ax[1][2])\n",
    "\n",
    "ax[0][0].set_title('Train')\n",
    "ax[0][0].set_xlim(left=-50)\n",
    "ax[0][1].set_title('Dev')\n",
    "ax[0][1].set_xlim(left=-50)\n",
    "ax[0][2].set_title('Test')\n",
    "ax[0][2].set_xlim(left=-50)\n",
    "\n",
    "ax[1][0].set_title('Train')\n",
    "ax[1][0].set_xlim(left=-10, right=2500)\n",
    "ax[1][1].set_title('Dev')\n",
    "ax[1][1].set_xlim(left=-10, right=2500)\n",
    "ax[1][2].set_title('Test')\n",
    "ax[1][2].set_xlim(left=-10, right=2500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"allenai/longformer-base-4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.longformer.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longformer.encoder.layer.11.attention.self.query.weight\n",
      "longformer.encoder.layer.11.attention.self.query.bias\n",
      "longformer.encoder.layer.11.attention.self.key.weight\n",
      "longformer.encoder.layer.11.attention.self.key.bias\n",
      "longformer.encoder.layer.11.attention.self.value.weight\n",
      "longformer.encoder.layer.11.attention.self.value.bias\n",
      "longformer.encoder.layer.11.attention.self.query_global.weight\n",
      "longformer.encoder.layer.11.attention.self.query_global.bias\n",
      "longformer.encoder.layer.11.attention.self.key_global.weight\n",
      "longformer.encoder.layer.11.attention.self.key_global.bias\n",
      "longformer.encoder.layer.11.attention.self.value_global.weight\n",
      "longformer.encoder.layer.11.attention.self.value_global.bias\n",
      "longformer.encoder.layer.11.attention.output.dense.weight\n",
      "longformer.encoder.layer.11.attention.output.dense.bias\n",
      "longformer.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "longformer.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "longformer.encoder.layer.11.intermediate.dense.weight\n",
      "longformer.encoder.layer.11.intermediate.dense.bias\n",
      "longformer.encoder.layer.11.output.dense.weight\n",
      "longformer.encoder.layer.11.output.dense.bias\n",
      "longformer.encoder.layer.11.output.LayerNorm.weight\n",
      "longformer.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 9451778 (6.36%)\n"
     ]
    }
   ],
   "source": [
    "def get_num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "trainable_params = get_num_parameters(model)\n",
    "print(f'Trainable parameters: {trainable_params} ({trainable_params / model.num_parameters():.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Measure performance before training/fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8180/8180 [08:46<00:00, 15.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.38856221156147674,\n",
       " 'accuracy': 0.6211347886215512,\n",
       " 'balanced_accuracy': 0.498585638605751,\n",
       " 'confusion_matrix': array([[   575,  97753],\n",
       "        [  1418, 162012]]),\n",
       " 'classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.29      0.01      0.01     98328\\n           1       0.62      0.99      0.77    163430\\n\\n    accuracy                           0.62    261758\\n   macro avg       0.46      0.50      0.39    261758\\nweighted avg       0.50      0.62      0.48    261758\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_performance_metrics(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cr\n",
    "    }\n",
    "\n",
    "\n",
    "def measure_initial_performance(model, tokenizer, df_test, device, max_length=512, batch_size=32):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_samples = df_test.text.tolist()\n",
    "\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, len(test_samples), batch_size)):\n",
    "        batch = test_samples[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            outputs.append(output.logits)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    performance = get_performance_metrics(df_test.label, predictions)\n",
    "    return performance\n",
    "\n",
    "\n",
    "initial_performance = measure_initial_performance(model, tokenizer, df_en_dev, DEVICE)\n",
    "initial_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_samples(model, tokenizer, df, device, max_length=512):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_samples = df.text.tolist()[:5]\n",
    "    inputs = tokenizer(\n",
    "        test_samples,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0314,  0.0181],\n",
      "        [-0.0319,  0.0190],\n",
      "        [-0.0338,  0.0227],\n",
      "        [-0.0318,  0.0197],\n",
      "        [-0.0323,  0.0195]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_samples(model, tokenizer, df_en_train, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6252, 0.3748])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (\n",
    "    1 / df_en_train.label.value_counts(normalize=True).sort_index()\n",
    ").tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_en_train)\n",
    "ds_dev = Dataset.from_pandas(df_en_dev)\n",
    "ds_test = Dataset.from_pandas(df_en_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "    num_rows: 427536\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_shuffled = ds_train.shuffle(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': ds_train_shuffled,\n",
    "    'dev': ds_dev,\n",
    "    'test': ds_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "        num_rows: 427536\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__'],\n",
       "        num_rows: 183231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f2e6e1c9c142f48333b3134ed97acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/427536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053ad4be3b6c424aa779111b996e0603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/183231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb87a114450402a8bf8751999a86ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/261758 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 2048\n",
    "# col_to_delete = ['source', 'sub_source', 'lang', 'model']\n",
    "\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    return tokenizer(ds['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_dataset, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 427536\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 183231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'macro_f1': f1_score(predictions, labels, average='macro'),\n",
    "        'accuracy': accuracy_score(predictions,labels),\n",
    "        'balanced_accuracy': balanced_accuracy_score(predictions, labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class WeightedCrossEntropyTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(\n",
    "                class_weights, dtype=torch.float32\n",
    "        ).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "# from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "# experiment = Experiment(\n",
    "#     api_key=comet_api_key,\n",
    "#     project_name=\"mgt-detection-coling-2025\",\n",
    "#     workspace=\"unibuc-phd\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"seed\": RANDOM_SEED,\n",
    "}\n",
    "\n",
    "# experiment.log_parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results/subtask_a/003_longformer-base-4096',\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=params[\"num_train_epochs\"],\n",
    "    weight_decay=params[\"weight_decay\"],\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    # report_to=\"comet_ml\",\n",
    "    seed=params[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24473/3314886341.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedCrossEntropyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['dev'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    # callbacks=[CometCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24657' max='26722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24657/26722 11:39:07 < 58:33, 0.59 it/s, Epoch 1.85/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.897694</td>\n",
       "      <td>0.384687</td>\n",
       "      <td>0.625189</td>\n",
       "      <td>0.625189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(ds_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(ds_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../results/subtask_a/003_longformer-base-4096/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def make_predictions(model, tokenizer, df, device, file_path=None, max_len=MAX_LEN, batch_size=16):\n",
    "    model.eval()\n",
    "\n",
    "    has_labels = (\"label\" in df.columns)\n",
    "\n",
    "    all_ids = []\n",
    "    all_true = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(df), batch_size)):\n",
    "            ids = df[i:i + batch_size].id.tolist()\n",
    "            texts = df[i:i + batch_size].text.tolist()\n",
    "            labels = None\n",
    "            if has_labels:\n",
    "                labels = df[i:i + batch_size].label.tolist()\n",
    "\n",
    "            inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            all_ids.extend(ids)\n",
    "            if has_labels:\n",
    "                all_true.extend(labels)\n",
    "            all_predictions.extend(preds)\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"label\": all_predictions,\n",
    "    })\n",
    "\n",
    "    if has_labels:\n",
    "        df_predictions[\"true\"] = all_true\n",
    "\n",
    "    if file_path is not None:\n",
    "        df_predictions.to_json(\n",
    "            file_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predictions = make_predictions(\n",
    "    trainer.model,\n",
    "    tokenizer,\n",
    "    df_en_test,\n",
    "    DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predictions.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for test dataset\n",
    "print(\n",
    "    f1_score(df_test_predictions.true, df_test_predictions.label, average='macro')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_submission_predictions = make_predictions(\n",
    "    trainer.model,\n",
    "    tokenizer,\n",
    "    df_en_test_no_label,\n",
    "    DEVICE,\n",
    "    \"../results/subtask_a/003_longformer-base-4096/subtask_a.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_submission_predictions.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "053ad4be3b6c424aa779111b996e0603": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_982414d585b446feada08577c3db6f82",
        "IPY_MODEL_31fadedd628a4da3a9cc3aeb61fbf190",
        "IPY_MODEL_3525539f6f9f4acab1f9a9f6baf499cc"
       ],
       "layout": "IPY_MODEL_3399a32d5aa64f77ad516c8be552ee39"
      }
     },
     "0e3f877c69794e908ae2b961b2c68db9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "130cdac4e43145eda61eb8290846cf4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "183c90bb436448a89d219a123f73718e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "184e506d13f4413cb519303c50603fca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "21f2e6e1c9c142f48333b3134ed97acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b70c6478abe4ef8a3a045fdc688638f",
        "IPY_MODEL_325bd434263342d7a443539625c6225a",
        "IPY_MODEL_627959da0aa14fedbb69c0b1ad6b2dfd"
       ],
       "layout": "IPY_MODEL_c93c8e649e4a499e9aff284889ab10d1"
      }
     },
     "29840bc233de41089a34c1bf6fb23dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2f7b95bf259b42a0b0388ce80a850195": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d5cdc816a31f41a6bb1d9530010ab6db",
       "max": 261758,
       "style": "IPY_MODEL_184e506d13f4413cb519303c50603fca",
       "value": 261758
      }
     },
     "31fadedd628a4da3a9cc3aeb61fbf190": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f53579da959648f2a405d23bd8762d0f",
       "max": 183231,
       "style": "IPY_MODEL_0e3f877c69794e908ae2b961b2c68db9",
       "value": 183231
      }
     },
     "325bd434263342d7a443539625c6225a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_183c90bb436448a89d219a123f73718e",
       "max": 427536,
       "style": "IPY_MODEL_8260179804d04d3d8bc5b11bd9842ca4",
       "value": 427536
      }
     },
     "3399a32d5aa64f77ad516c8be552ee39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3525539f6f9f4acab1f9a9f6baf499cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_69c7cf47257b4cb28adcd3e1ba1bff10",
       "style": "IPY_MODEL_130cdac4e43145eda61eb8290846cf4c",
       "value": " 183231/183231 [00:44&lt;00:00, 4325.10 examples/s]"
      }
     },
     "3c936fa1ee51411ca30734bf2315f3bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4775a6ae30cf48c4b9878d0bf146713d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b70c6478abe4ef8a3a045fdc688638f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4775a6ae30cf48c4b9878d0bf146713d",
       "style": "IPY_MODEL_9e6cdb4041454ed190d8240ea3502c06",
       "value": "Map: 100%"
      }
     },
     "547c97c36013477597d06b0978b3e0f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "627959da0aa14fedbb69c0b1ad6b2dfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cc09baa7f0b44ffda7bbb53b53aac8ee",
       "style": "IPY_MODEL_547c97c36013477597d06b0978b3e0f0",
       "value": " 427536/427536 [02:05&lt;00:00, 3424.08 examples/s]"
      }
     },
     "69c7cf47257b4cb28adcd3e1ba1bff10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7fb87a114450402a8bf8751999a86ed0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9ca9c605728e4a0898f4c05da73c1abe",
        "IPY_MODEL_2f7b95bf259b42a0b0388ce80a850195",
        "IPY_MODEL_b23c722084e64c96980e423759313c76"
       ],
       "layout": "IPY_MODEL_b3241fd9b87e4d20a498a61641001ed7"
      }
     },
     "8260179804d04d3d8bc5b11bd9842ca4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "876c05c002b14593a814e3aa6890c7b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "883a0e3de22f44eba3476664a2487bb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "965d9b3b4a254b51a4e9ebcd4feb0faf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "982414d585b446feada08577c3db6f82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_876c05c002b14593a814e3aa6890c7b4",
       "style": "IPY_MODEL_965d9b3b4a254b51a4e9ebcd4feb0faf",
       "value": "Map: 100%"
      }
     },
     "9ca9c605728e4a0898f4c05da73c1abe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_29840bc233de41089a34c1bf6fb23dce",
       "style": "IPY_MODEL_3c936fa1ee51411ca30734bf2315f3bb",
       "value": "Map: 100%"
      }
     },
     "9e6cdb4041454ed190d8240ea3502c06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b23c722084e64c96980e423759313c76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_883a0e3de22f44eba3476664a2487bb2",
       "style": "IPY_MODEL_bd5a2e6eb8c04c2d87025a69b0759f51",
       "value": " 261758/261758 [01:04&lt;00:00, 3263.51 examples/s]"
      }
     },
     "b3241fd9b87e4d20a498a61641001ed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd5a2e6eb8c04c2d87025a69b0759f51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c93c8e649e4a499e9aff284889ab10d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cc09baa7f0b44ffda7bbb53b53aac8ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d5cdc816a31f41a6bb1d9530010ab6db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f53579da959648f2a405d23bd8762d0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
